{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文用一个简单的数值例子，说明了 LayerNorm 的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 9., 3., 0.],\n",
       "         [3., 9., 7., 3.],\n",
       "         [7., 3., 1., 6.]],\n",
       "\n",
       "        [[6., 9., 8., 6.],\n",
       "         [6., 8., 4., 3.],\n",
       "         [6., 9., 1., 4.]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.randint(0, 10, (2, 3, 4)).float()\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看 LayerNorm 的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `normalized_shape` 指定为最后一维特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们以 `normalized_shape` 为 `4` 为例，验证 LayerNorm 的效果。\n",
    "\n",
    "`normalized_shape` 为 `4` 时，意味着对最后一个维度上的 4 个元素进行标准化。\n",
    "\n",
    "以上述输入张量为例，则需要对 `[4., 9., 3., 0.]` 这一行数据进行标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.5430, -0.3086, -1.2344],\n",
       "         [-0.9622,  1.3471,  0.5773, -0.9622],\n",
       "         [ 1.1531, -0.5241, -1.3628,  0.7338]],\n",
       "\n",
       "        [[-0.9622,  1.3471,  0.5773, -0.9622],\n",
       "         [ 0.3906,  1.4321, -0.6509, -1.1717],\n",
       "         [ 0.3430,  1.3720, -1.3720, -0.3430]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor = layer_norm(input_tensor)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.0000],\n",
       "         [5.5000],\n",
       "         [4.2500]],\n",
       "\n",
       "        [[7.2500],\n",
       "         [5.2500],\n",
       "         [5.0000]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.mean(dim=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2404],\n",
       "         [2.5981],\n",
       "         [2.3848]],\n",
       "\n",
       "        [[1.2990],\n",
       "         [1.9203],\n",
       "         [2.9155]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.std(dim=2, unbiased=False, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.5430, -0.3086, -1.2344],\n",
       "         [-0.9622,  1.3471,  0.5773, -0.9622],\n",
       "         [ 1.1531, -0.5241, -1.3628,  0.7338]],\n",
       "\n",
       "        [[-0.9622,  1.3471,  0.5773, -0.9622],\n",
       "         [ 0.3906,  1.4321, -0.6509, -1.1717],\n",
       "         [ 0.3430,  1.3720, -1.3720, -0.3430]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_tensor - input_tensor.mean(dim=2, keepdim=True)) / (\n",
    "    input_tensor.std(dim=2, unbiased=False, keepdim=True) + 1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `normalized_shape` 指定为最后两维特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们以 `normalized_shape` 为 `[3, 4]` 为例，验证 LayerNorm 的效果。\n",
    "\n",
    "`normalized_shape` 为 `[3, 4]` 时，意味着对最后两个维度上的 3 行 4 列的元素进行标准化。\n",
    "\n",
    "以上述输入张量为例，则需要对\n",
    "\n",
    "```\n",
    "[[4., 9., 3., 0.],\n",
    " [3., 9., 7., 3.],\n",
    " [7., 3., 1., 6.]]\n",
    "```\n",
    "\n",
    "进行标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2053,  1.5541, -0.5571, -1.6128],\n",
       "         [-0.5571,  1.5541,  0.8504, -0.5571],\n",
       "         [ 0.8504, -0.5571, -1.2609,  0.4985]],\n",
       "\n",
       "        [[ 0.0702,  1.3335,  0.9124,  0.0702],\n",
       "         [ 0.0702,  0.9124, -0.7720, -1.1932],\n",
       "         [ 0.0702,  1.3335, -2.0354, -0.7720]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor = layer_norm(input_tensor)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.5833]],\n",
       "\n",
       "        [[5.8333]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.mean(dim=(1, 2), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.8419]],\n",
       "\n",
       "        [[2.3746]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.std(dim=(1, 2), unbiased=False, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2053,  1.5541, -0.5571, -1.6128],\n",
       "         [-0.5571,  1.5541,  0.8504, -0.5571],\n",
       "         [ 0.8504, -0.5571, -1.2609,  0.4985]],\n",
       "\n",
       "        [[ 0.0702,  1.3335,  0.9124,  0.0702],\n",
       "         [ 0.0702,  0.9124, -0.7720, -1.1932],\n",
       "         [ 0.0702,  1.3335, -2.0354, -0.7720]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_tensor - input_tensor.mean(dim=(1, 2), keepdim=True)) / (\n",
    "    input_tensor.std(dim=(1, 2), unbiased=False, keepdim=True) + 1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\gamma$ 和 $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 LayerNorm 中，$\\gamma$ 和 $\\beta$ 是可学习的参数，用于对标准化后的数据进行缩放和平移。\n",
    "\n",
    "$\\gamma$ 和 $\\beta$ 的初始值为 1 和 0，维数和 `normalized_shape` 一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = nn.LayerNorm(4)\n",
    "print(layer_norm.weight)\n",
    "print(layer_norm.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = nn.LayerNorm([3, 4])\n",
    "print(layer_norm.weight)\n",
    "print(layer_norm.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
